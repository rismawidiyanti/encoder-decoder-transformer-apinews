{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK/+1qJZSXYxsQQV9UCFTy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install rouge rouge-score nltk pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SKZ4MEV3D19T",
        "outputId": "ecf02047-3b3e-4c99-a961-1b8b8e7c7cc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ea873c29d5d680e317eaf111ad1a5d88d66454b4255a3008ddca0cf256220552\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge, rouge-score\n",
            "Successfully installed rouge-1.0.1 rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dot, Activation, Reshape, Concatenate, Embedding, LayerNormalization, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LbfRb3QEk1g",
        "outputId": "91fea3b6-1cc0-4f9c-da63-9ea6ed416c61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "\n",
        "# Konfigurasi API\n",
        "API_KEY = \"3ad8deb6cd0f4cfb9cb1747057f34e78\"  # Gunakan API key yang Anda berikan\n",
        "BASE_URL = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "# Fungsi untuk mengecek API key dan koneksi\n",
        "def test_api_key(api_key):\n",
        "    params = {\n",
        "        \"q\": \"test\",\n",
        "        \"pageSize\": 1,\n",
        "        \"apiKey\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        if response.status_code == 200 and data.get(\"status\") == \"ok\":\n",
        "            print(f\"✅ API Key valid. Status: {data.get('status')}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ API error: {data.get('message', 'Unknown error')}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Exception: {e}\")\n",
        "        return False\n",
        "\n",
        "# Fungsi untuk mengambil artikel dengan paging\n",
        "def fetch_all_articles(query, api_key, from_date=None, to_date=None, max_pages=10):\n",
        "    all_articles = []\n",
        "    total_available = 0\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"pageSize\": 100,  # Maksimum per halaman\n",
        "            \"page\": page,\n",
        "            \"apiKey\": api_key,\n",
        "            \"sortBy\": \"publishedAt\"\n",
        "        }\n",
        "\n",
        "        if from_date:\n",
        "            params[\"from\"] = from_date\n",
        "        if to_date:\n",
        "            params[\"to\"] = to_date\n",
        "\n",
        "        print(f\"Fetching page {page} for query '{query}'...\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(BASE_URL, params=params)\n",
        "            data = response.json()\n",
        "\n",
        "            if response.status_code == 200 and data.get(\"status\") == \"ok\":\n",
        "                articles = data.get(\"articles\", [])\n",
        "                all_articles.extend(articles)\n",
        "\n",
        "                # Simpan total hasil yang tersedia\n",
        "                if page == 1:\n",
        "                    total_available = data.get(\"totalResults\", 0)\n",
        "                    print(f\"Total results available: {total_available}\")\n",
        "\n",
        "                print(f\"Retrieved {len(articles)} articles on page {page}\")\n",
        "\n",
        "                # Jika kurang dari 100 artikel, berarti ini halaman terakhir\n",
        "                if len(articles) < 100:\n",
        "                    print(f\"Reached last page with {len(articles)} articles\")\n",
        "                    break\n",
        "\n",
        "                # Sleep untuk menghindari rate limiting\n",
        "                time.sleep(0.5)\n",
        "            else:\n",
        "                print(f\"Error on page {page}: {data.get('message', 'Unknown error')}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Exception on page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"Total articles collected: {len(all_articles)} out of {total_available} available\")\n",
        "    return all_articles\n",
        "\n",
        "# Cek API key\n",
        "if not test_api_key(API_KEY):\n",
        "    print(\"Please check your API key and try again.\")\n",
        "else:\n",
        "    # Tanggal (30 hari terakhir)\n",
        "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    start_date = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    print(f\"\\nCollecting news articles from {start_date} to {end_date}\")\n",
        "\n",
        "    # Daftar query populer\n",
        "    queries = [\n",
        "        \"technology\",\n",
        "        \"politics\", \"business\", \"science\", \"health\",\n",
        "        \"sports\", \"entertainment\", \"education\", \"environment\", \"finance\",\n",
        "        \"medicine\", \"innovation\", \"artificial intelligence\", \"climate change\", \"space exploration\",\n",
        "        \"bitcoin\", \"olympics\", \"COVID\"\n",
        "    ]\n",
        "\n",
        "    # Kumpulkan artikel dari semua query\n",
        "    all_data = []\n",
        "\n",
        "    for query in queries:\n",
        "        articles = fetch_all_articles(query, API_KEY, from_date=start_date, to_date=end_date, max_pages=5)\n",
        "\n",
        "        # Tambahkan query sebagai tag\n",
        "        for article in articles:\n",
        "            article[\"query\"] = query\n",
        "\n",
        "        all_data.extend(articles)\n",
        "        print(f\"Collected {len(articles)} articles for '{query}'. Total: {len(all_data)}\")\n",
        "\n",
        "        # Cek rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Ubah ke DataFrame\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Hapus duplikat berdasarkan URL\n",
        "    original_count = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"url\"])\n",
        "    print(f\"Removed {original_count - len(df)} duplicate articles. Remaining: {len(df)}\")\n",
        "\n",
        "    # Periksa kolom apa saja yang tersedia\n",
        "    print(\"\\nColumns available in dataset:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Periksa konten artikel\n",
        "    content_available = 'content' in df.columns\n",
        "    if content_available:\n",
        "        print(f\"\\nContent column available: {content_available}\")\n",
        "        print(f\"Articles with non-null content: {df['content'].notna().sum()} of {len(df)}\")\n",
        "\n",
        "        # Tampilkan panjang konten\n",
        "        if df['content'].notna().any():\n",
        "            df['content_length'] = df['content'].fillna('').apply(len)\n",
        "            print(f\"Average content length: {df['content_length'].mean():.1f} characters\")\n",
        "            print(f\"Min content length: {df['content_length'].min()} characters\")\n",
        "            print(f\"Max content length: {df['content_length'].max()} characters\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: 'content' column not available!\")\n",
        "        print(\"Using 'description' as content instead...\")\n",
        "        df['content'] = df['description']\n",
        "\n",
        "    # Simpan hasil\n",
        "    df.to_csv(\"newsapi_articles.csv\", index=False)\n",
        "    print(f\"\\nSaved {len(df)} articles to 'newsapi_articles.csv'\")\n",
        "\n",
        "    # Tampilkan beberapa contoh\n",
        "    print(\"\\nSample articles:\")\n",
        "    for i, row in df.sample(min(3, len(df))).iterrows():\n",
        "        print(f\"\\n--- Article {i} ---\")\n",
        "        print(f\"Title: {row.get('title', 'N/A')}\")\n",
        "        print(f\"Source: {row.get('source', {}).get('name', 'N/A')}\")\n",
        "        print(f\"Published: {row.get('publishedAt', 'N/A')}\")\n",
        "        content = row.get('content', 'N/A')\n",
        "        if isinstance(content, str) and len(content) > 100:\n",
        "            content = content[:100] + \"...\"\n",
        "        print(f\"Content: {content}\")\n",
        "\n",
        "# Setelah pengumpulan data, lanjutkan ke preprocessing\n",
        "print(\"\\nNow you can continue with preprocessing steps...\")"
      ],
      "metadata": {
        "id": "HQ5sG5J3pvZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19CUyFT1Dao0",
        "outputId": "739099e9-d732-43c7-f3cc-97a029821ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1327\n",
            "Training samples: 1061\n",
            "Test samples: 266\n",
            "\n",
            "Example Input-Output Pair:\n",
            "Input (Article): shannon sharpe pro football hall famer espn analyst grappling serious allegations second sexual assault lawsuit surfaced time different accuser according 2307 chars...\n",
            "Output (Title): shannon sharpe hit second sexual assault lawsuit new accuser\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Select relevant columns: content (article text) and title\n",
        "    df = df[['content', 'title']].dropna()  # Drop rows with missing content or title\n",
        "    return df\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    # Convert to string if not already\n",
        "    text = str(text)\n",
        "    # Remove special characters, URLs, and extra whitespace\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
        "    text = text.lower().strip()  # Convert to lowercase and strip\n",
        "    return text\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "def tokenize_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(df):\n",
        "    # Clean content and title\n",
        "    df['cleaned_content'] = df['content'].apply(clean_text).apply(tokenize_text)\n",
        "    df['cleaned_title'] = df['title'].apply(clean_text).apply(tokenize_text)\n",
        "\n",
        "    # Filter out empty strings after cleaning\n",
        "    df = df[df['cleaned_content'] != '']\n",
        "    df = df[df['cleaned_title'] != '']\n",
        "\n",
        "    # Create input-output pairs\n",
        "    input_texts = df['cleaned_content'].tolist()\n",
        "    target_texts = df['cleaned_title'].tolist()\n",
        "\n",
        "    return input_texts, target_texts\n",
        "\n",
        "# Main function to process data\n",
        "def main(file_path):\n",
        "    # Load data\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    # Preprocess data\n",
        "    input_texts, target_texts = preprocess_data(df)\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
        "        input_texts, target_texts, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save preprocessed data (optional)\n",
        "    with open('train_inputs.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(train_inputs))\n",
        "    with open('train_targets.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(train_targets))\n",
        "    with open('test_inputs.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(test_inputs))\n",
        "    with open('test_targets.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(test_targets))\n",
        "\n",
        "    print(f\"Total samples: {len(input_texts)}\")\n",
        "    print(f\"Training samples: {len(train_inputs)}\")\n",
        "    print(f\"Test samples: {len(test_inputs)}\")\n",
        "\n",
        "    # Example of input-output pair\n",
        "    print(\"\\nExample Input-Output Pair:\")\n",
        "    print(f\"Input (Article): {train_inputs[0][:200]}...\")\n",
        "    print(f\"Output (Title): {train_targets[0]}\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = 'newsapi_articles.csv'  # Adjust path if needed\n",
        "    main(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "MAX_VOCAB_SIZE = 20000  # Maximum vocabulary size\n",
        "EMBEDDING_DIM = 128     # Embedding dimension\n",
        "LSTM_UNITS = 256        # Number of LSTM units\n",
        "MAX_ARTICLE_LEN = 200   # Maximum length of article (input)\n",
        "MAX_TITLE_LEN = 20      # Maximum length of title (output)\n",
        "\n",
        "# Load preprocessed data\n",
        "def load_preprocessed_data(train_input_path, train_target_path, test_input_path, test_target_path):\n",
        "    with open(train_input_path, 'r', encoding='utf-8') as f:\n",
        "        train_inputs = f.read().splitlines()\n",
        "    with open(train_target_path, 'r', encoding='utf-8') as f:\n",
        "        train_targets = f.read().splitlines()\n",
        "    with open(test_input_path, 'r', encoding='utf-8') as f:\n",
        "        test_inputs = f.read().splitlines()\n",
        "    with open(test_target_path, 'r', encoding='utf-8') as f:\n",
        "        test_targets = f.read().splitlines()\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def prepare_sequences(input_texts, target_texts, max_input_len, max_target_len):\n",
        "    # Tokenize input (articles)\n",
        "    input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "    input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "    input_padded = pad_sequences(input_sequences, maxlen=max_input_len, padding='post', truncating='post')\n",
        "\n",
        "    # Tokenize target (titles)\n",
        "    target_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "    target_padded = pad_sequences(target_sequences, maxlen=max_target_len, padding='post', truncating='post')\n",
        "\n",
        "    # Prepare decoder input and output (shifted by one for teacher forcing)\n",
        "    decoder_input_data = target_padded[:, :-1]\n",
        "    decoder_target_data = target_padded[:, 1:]\n",
        "\n",
        "    return input_padded, decoder_input_data, decoder_target_data, input_tokenizer, target_tokenizer\n",
        "\n",
        "# Build LSTM encoder-decoder model\n",
        "def build_model(input_vocab_size, target_vocab_size, embedding_dim, lstm_units, max_target_len):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load preprocessed data\n",
        "    train_inputs, train_targets, test_inputs, test_targets = load_preprocessed_data(\n",
        "        'train_inputs.txt', 'train_targets.txt', 'test_inputs.txt', 'test_targets.txt'\n",
        "    )\n",
        "\n",
        "    # Prepare sequences\n",
        "    input_padded, decoder_input_data, decoder_target_data, input_tokenizer, target_tokenizer = prepare_sequences(\n",
        "        train_inputs + test_inputs, train_targets + test_targets, MAX_ARTICLE_LEN, MAX_TITLE_LEN\n",
        "    )\n",
        "\n",
        "    # Split data back into train and test\n",
        "    train_inputs_padded, test_inputs_padded, train_decoder_inputs, test_decoder_inputs, train_decoder_targets, test_decoder_targets = train_test_split(\n",
        "        input_padded, decoder_input_data, decoder_target_data, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    input_vocab_size = min(len(input_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "    target_vocab_size = min(len(target_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "    # Build and train model\n",
        "    model = build_model(input_vocab_size, target_vocab_size, EMBEDDING_DIM, LSTM_UNITS, MAX_TITLE_LEN - 1)\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        [train_inputs_padded, train_decoder_inputs],\n",
        "        train_decoder_targets,\n",
        "        batch_size=64,\n",
        "        epochs=10,\n",
        "        validation_data=([test_inputs_padded, test_decoder_inputs], test_decoder_targets)\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    model.save('lstm_encoder_decoder_model.h5')\n",
        "\n",
        "    # Save tokenizers (optional)\n",
        "    import pickle\n",
        "    with open('input_tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(input_tokenizer, f)\n",
        "    with open('target_tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(target_tokenizer, f)\n",
        "\n",
        "    print(\"Model training completed and saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "Pd8jrxBWZcOU",
        "outputId": "a589025e-b935-4f5a-bb02-57060d4c784f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,479,296\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m778,624\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m394,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m394,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,563,331\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m6083\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,479,296</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">778,624</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,563,331</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">6083</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,609,731\u001b[0m (17.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,609,731</span> (17.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,609,731\u001b[0m (17.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,609,731</span> (17.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.4634 - loss: 7.9371 - val_accuracy: 0.5708 - val_loss: 4.3166\n",
            "Epoch 2/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.5764 - loss: 4.0585 - val_accuracy: 0.5708 - val_loss: 4.2054\n",
            "Epoch 3/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.7219 - val_accuracy: 0.5708 - val_loss: 4.1851\n",
            "Epoch 4/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.6078 - val_accuracy: 0.5714 - val_loss: 4.1816\n",
            "Epoch 5/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.5766 - loss: 3.5439 - val_accuracy: 0.5712 - val_loss: 4.1999\n",
            "Epoch 6/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.5034 - val_accuracy: 0.5706 - val_loss: 4.2378\n",
            "Epoch 7/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.5764 - loss: 3.4717 - val_accuracy: 0.5702 - val_loss: 4.2649\n",
            "Epoch 8/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.5763 - loss: 3.4409 - val_accuracy: 0.5700 - val_loss: 4.2990\n",
            "Epoch 9/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - accuracy: 0.5766 - loss: 3.4104 - val_accuracy: 0.5706 - val_loss: 4.3356\n",
            "Epoch 10/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.5766 - loss: 3.3799 - val_accuracy: 0.5706 - val_loss: 4.3646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "MAX_VOCAB_SIZE = 20000  # Maximum vocabulary size\n",
        "EMBEDDING_DIM = 128     # Embedding dimension\n",
        "ATTENTION_UNITS = 256   # Number of units for attention\n",
        "MAX_ARTICLE_LEN = 200   # Maximum length of article (input)\n",
        "MAX_TITLE_LEN = 20      # Maximum length of title (output)\n",
        "\n",
        "# Load preprocessed data\n",
        "def load_preprocessed_data(train_input_path, train_target_path, test_input_path, test_target_path):\n",
        "    with open(train_input_path, 'r', encoding='utf-8') as f:\n",
        "        train_inputs = f.read().splitlines()\n",
        "    with open(train_target_path, 'r', encoding='utf-8') as f:\n",
        "        train_targets = f.read().splitlines()\n",
        "    with open(test_input_path, 'r', encoding='utf-8') as f:\n",
        "        test_inputs = f.read().splitlines()\n",
        "    with open(test_target_path, 'r', encoding='utf-8') as f:\n",
        "        test_targets = f.read().splitlines()\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def prepare_sequences(input_texts, target_texts, max_input_len, max_target_len):\n",
        "    # Tokenize input (articles)\n",
        "    input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "    input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "    input_padded = pad_sequences(input_sequences, maxlen=max_input_len, padding='post', truncating='post')\n",
        "\n",
        "    # Tokenize target (titles)\n",
        "    target_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "    target_padded = pad_sequences(target_sequences, maxlen=max_target_len, padding='post', truncating='post')\n",
        "\n",
        "    # Prepare decoder input and output (shifted by one for teacher forcing)\n",
        "    decoder_input_data = target_padded[:, :-1]\n",
        "    decoder_target_data = target_padded[:, 1:]\n",
        "\n",
        "    return input_padded, decoder_input_data, decoder_target_data, input_tokenizer, target_tokenizer\n",
        "\n",
        "# Build encoder-decoder with attention\n",
        "def build_attention_model(input_vocab_size, target_vocab_size, embedding_dim, attention_units, max_target_len):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_dense = Dense(attention_units, activation='relu')(encoder_embedding)  # Dense layer for encoding\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_dense = Dense(attention_units, activation='relu')(decoder_embedding)  # Dense layer for decoding\n",
        "\n",
        "    # Attention mechanism (Dot Product Attention)\n",
        "    attention_score = Dot(axes=(2, 2))([decoder_dense, encoder_dense])  # Compute alignment scores\n",
        "    attention_weights = Activation('softmax')(attention_score)  # Normalize scores\n",
        "    context_vector = Dot(axes=(2, 1))([attention_weights, encoder_dense])  # Weighted sum of encoder outputs\n",
        "\n",
        "    # Combine context vector with decoder output\n",
        "    decoder_combined = Concatenate(axis=-1)([context_vector, decoder_dense])\n",
        "    decoder_output = Dense(attention_units, activation='relu')(decoder_combined)\n",
        "    decoder_final = Dense(target_vocab_size, activation='softmax')(decoder_output)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_final)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load preprocessed data\n",
        "    train_inputs, train_targets, test_inputs, test_targets = load_preprocessed_data(\n",
        "        'train_inputs.txt', 'train_targets.txt', 'test_inputs.txt', 'test_targets.txt'\n",
        "    )\n",
        "\n",
        "    # Prepare sequences\n",
        "    input_padded, decoder_input_data, decoder_target_data, input_tokenizer, target_tokenizer = prepare_sequences(\n",
        "        train_inputs + test_inputs, train_targets + test_targets, MAX_ARTICLE_LEN, MAX_TITLE_LEN\n",
        "    )\n",
        "\n",
        "    # Split data back into train and test\n",
        "    train_inputs_padded, test_inputs_padded, train_decoder_inputs, test_decoder_inputs, train_decoder_targets, test_decoder_targets = train_test_split(\n",
        "        input_padded, decoder_input_data, decoder_target_data, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    input_vocab_size = min(len(input_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "    target_vocab_size = min(len(target_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "    # Build and train model\n",
        "    model = build_attention_model(input_vocab_size, target_vocab_size, EMBEDDING_DIM, ATTENTION_UNITS, MAX_TITLE_LEN - 1)\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        [train_inputs_padded, train_decoder_inputs],\n",
        "        train_decoder_targets,\n",
        "        batch_size=64,\n",
        "        epochs=10,\n",
        "        validation_data=([test_inputs_padded, test_decoder_inputs], test_decoder_targets)\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    model.save('encoder_decoder_attention_model.h5')\n",
        "\n",
        "    # Save tokenizers\n",
        "    with open('input_tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(input_tokenizer, f)\n",
        "    with open('target_tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(target_tokenizer, f)\n",
        "\n",
        "    print(\"Model training completed and saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g18ClpLZZsgm",
        "outputId": "6a4e6439-09eb-4ce8-eb74-13d222150264"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_23      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_23        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m778,624\u001b[0m │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_22        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,479,296\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m33,024\u001b[0m │ embedding_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m33,024\u001b[0m │ embedding_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;45mNone\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dot_1 (\u001b[38;5;33mDot\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │                   │            │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ dot_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m131,328\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,563,331\u001b[0m │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                     │ \u001b[38;5;34m6083\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_23      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_23        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">778,624</span> │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_22        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,479,296</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ embedding_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ embedding_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dot_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │                   │            │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,563,331</span> │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">6083</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,018,627\u001b[0m (15.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,018,627</span> (15.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,018,627\u001b[0m (15.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,018,627</span> (15.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.4639 - loss: 7.8023 - val_accuracy: 0.5708 - val_loss: 4.5687\n",
            "Epoch 2/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.9037 - val_accuracy: 0.5708 - val_loss: 4.1017\n",
            "Epoch 3/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.6825 - val_accuracy: 0.5708 - val_loss: 4.4477\n",
            "Epoch 4/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.6052 - val_accuracy: 0.5708 - val_loss: 4.3373\n",
            "Epoch 5/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.5553 - val_accuracy: 0.5708 - val_loss: 4.3771\n",
            "Epoch 6/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.5288 - val_accuracy: 0.5708 - val_loss: 4.3774\n",
            "Epoch 7/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.4757 - val_accuracy: 0.5708 - val_loss: 4.4549\n",
            "Epoch 8/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.4337 - val_accuracy: 0.5708 - val_loss: 4.6246\n",
            "Epoch 9/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.5764 - loss: 3.4095 - val_accuracy: 0.5659 - val_loss: 4.5817\n",
            "Epoch 10/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.5764 - loss: 3.3943 - val_accuracy: 0.5663 - val_loss: 4.6342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "MAX_VOCAB_SIZE = 20000  # Maximum vocabulary size\n",
        "EMBEDDING_DIM = 128     # Embedding dimension\n",
        "NUM_HEADS = 4           # Number of attention heads\n",
        "FF_DIM = 512            # Feed-forward network dimension\n",
        "NUM_LAYERS = 2          # Number of transformer layers\n",
        "DROPOUT_RATE = 0.1      # Dropout rate\n",
        "MAX_ARTICLE_LEN = 200   # Maximum length of article (input)\n",
        "MAX_TITLE_LEN = 20      # Maximum length of title (output)\n",
        "\n",
        "# Multi-Head Attention Layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "        self.dense = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention = tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(self.depth, tf.float32))\n",
        "        if mask is not None:\n",
        "            scaled_attention += (mask * -1e9)\n",
        "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "        return self.dense(output)\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Transformer Decoder Layer\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "        self.dropout3 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "# Transformer Model\n",
        "def build_transformer_model(input_vocab_size, target_vocab_size, d_model, num_heads, ff_dim, num_layers, max_target_len, dropout_rate=0.1):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    enc_embedding = Embedding(input_vocab_size, d_model)(encoder_inputs)\n",
        "    enc_output = enc_embedding\n",
        "    for _ in range(num_layers):\n",
        "        enc_output = EncoderLayer(d_model, num_heads, ff_dim, dropout_rate)(enc_output, training=True)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    dec_embedding = Embedding(target_vocab_size, d_model)(decoder_inputs)\n",
        "    dec_output = dec_embedding\n",
        "    for _ in range(num_layers):\n",
        "        dec_output = DecoderLayer(d_model, num_heads, ff_dim, dropout_rate)(\n",
        "            dec_output, enc_output, training=True, look_ahead_mask=None, padding_mask=None\n",
        "        )\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(target_vocab_size, activation='softmax')(dec_output)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Load preprocessed data\n",
        "def load_preprocessed_data(train_input_path, train_target_path, test_input_path, test_target_path):\n",
        "    with open(train_input_path, 'r', encoding='utf-8') as f:\n",
        "        train_inputs = f.read().splitlines()\n",
        "    with open(train_target_path, 'r', encoding='utf-8') as f:\n",
        "        train_targets = f.read().splitlines()\n",
        "    with open(test_input_path, 'r', encoding='utf-8') as f:\n",
        "        test_inputs = f.read().splitlines()\n",
        "    with open(test_target_path, 'r', encoding='utf-8') as f:\n",
        "        test_targets = f.read().splitlines()\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def prepare_sequences(input_texts, target_texts, max_input_len, max_target_len):\n",
        "    input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "    input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "    input_padded = pad_sequences(input_sequences, maxlen=max_input_len, padding='post', truncating='post')\n",
        "\n",
        "    target_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "    target_padded = pad_sequences(target_sequences, maxlen=max_target_len, padding='post', truncating='post')\n",
        "\n",
        "    decoder_input_data = target_padded[:, :-1]\n",
        "    decoder_target_data = target_padded[:, 1:]\n",
        "\n",
        "    return input_padded, decoder_input_data, decoder_target_data, input_tokenizer, target_tokenizer\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load preprocessed data\n",
        "    train_inputs, train_targets, test_inputs, test_targets = load_preprocessed_data(\n",
        "        'train_inputs.txt', 'train_targets.txt', 'test_inputs.txt', 'test_targets.txt'\n",
        "    )\n",
        "\n",
        "    # Prepare sequences\n",
        "    input_padded, decoder_input_data, decoder_target_data, input_tokenizer, target_tokenizer = prepare_sequences(\n",
        "        train_inputs + test_inputs, train_targets + test_targets, MAX_ARTICLE_LEN, MAX_TITLE_LEN\n",
        "    )\n",
        "\n",
        "    # Split data back into train and test\n",
        "    train_inputs_padded, test_inputs_padded, train_decoder_inputs, test_decoder_inputs, train_decoder_targets, test_decoder_targets = train_test_split(\n",
        "        input_padded, decoder_input_data, decoder_target_data, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    input_vocab_size = min(len(input_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "    target_vocab_size = min(len(target_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "    # Build and train model\n",
        "    model = build_transformer_model(\n",
        "        input_vocab_size, target_vocab_size, EMBEDDING_DIM, NUM_HEADS, FF_DIM, NUM_LAYERS, MAX_TITLE_LEN - 1, DROPOUT_RATE\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        [train_inputs_padded, train_decoder_inputs],\n",
        "        train_decoder_targets,\n",
        "        batch_size=64,\n",
        "        epochs=10,\n",
        "        validation_data=([test_inputs_padded, test_decoder_inputs], test_decoder_targets)\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    model.save('transformer_model.h5')\n",
        "\n",
        "    # Save tokenizers\n",
        "    with open('input_tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(input_tokenizer, f)\n",
        "    with open('target_tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(target_tokenizer, f)\n",
        "\n",
        "    print(\"Model training completed and saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "REPUUEWkgQyn",
        "outputId": "2c4c2e5a-fec0-4dfc-ce0a-5f72898fde35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_25      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_25        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,479,296\u001b[0m │ input_layer_25[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_28      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_layer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,272\u001b[0m │ embedding_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mEncoderLayer\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_26        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m778,624\u001b[0m │ input_layer_28[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_layer_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,272\u001b[0m │ encoder_layer[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEncoderLayer\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_layer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m264,576\u001b[0m │ embedding_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDecoderLayer\u001b[0m)      │                   │            │ encoder_layer_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_layer_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m264,576\u001b[0m │ decoder_layer[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDecoderLayer\u001b[0m)      │                   │            │ encoder_layer_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m784,707\u001b[0m │ decoder_layer_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m6083\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_25      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_25        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,479,296</span> │ input_layer_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_28      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_layer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ embedding_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_26        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">778,624</span> │ input_layer_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_layer_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ encoder_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_layer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">264,576</span> │ embedding_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderLayer</span>)      │                   │            │ encoder_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_layer_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">264,576</span> │ decoder_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderLayer</span>)      │                   │            │ encoder_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">784,707</span> │ decoder_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">6083</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,968,323\u001b[0m (15.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,968,323</span> (15.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,968,323\u001b[0m (15.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,968,323</span> (15.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.4640 - loss: 7.4619 - val_accuracy: 0.5708 - val_loss: 5.8932\n",
            "Epoch 2/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 5.3445 - val_accuracy: 0.5708 - val_loss: 4.6094\n",
            "Epoch 3/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 4.2004 - val_accuracy: 0.5708 - val_loss: 4.5063\n",
            "Epoch 4/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.9723 - val_accuracy: 0.5708 - val_loss: 4.1491\n",
            "Epoch 5/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.6965 - val_accuracy: 0.5708 - val_loss: 4.0809\n",
            "Epoch 6/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.6393 - val_accuracy: 0.5708 - val_loss: 4.2215\n",
            "Epoch 7/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.5768 - loss: 3.6389 - val_accuracy: 0.5708 - val_loss: 4.1754\n",
            "Epoch 8/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 3s/step - accuracy: 0.5764 - loss: 3.5692 - val_accuracy: 0.5708 - val_loss: 4.0864\n",
            "Epoch 9/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step - accuracy: 0.5773 - loss: 3.3007 - val_accuracy: 0.5693 - val_loss: 4.0134\n",
            "Epoch 10/10\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.5867 - loss: 3.0556 - val_accuracy: 0.5732 - val_loss: 3.9654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters (sesuaikan dengan pelatihan)\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "MAX_ARTICLE_LEN = 200\n",
        "MAX_TITLE_LEN = 20\n",
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 256\n",
        "ATTENTION_UNITS = 256\n",
        "NUM_HEADS = 4\n",
        "FF_DIM = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "# Clean text function (sama seperti preprocessing)\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "def tokenize_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess article\n",
        "def preprocess_article(article, input_tokenizer):\n",
        "    cleaned_article = clean_text(article)\n",
        "    tokenized_article = tokenize_text(cleaned_article)\n",
        "    sequence = input_tokenizer.texts_to_sequences([tokenized_article])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=MAX_ARTICLE_LEN, padding='post', truncating='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# Definisikan kembali kelas kustom untuk Transformer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        scaled_attention = tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(self.depth, tf.float32))\n",
        "        if mask is not None:\n",
        "            scaled_attention += (mask * -1e9)\n",
        "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "        return self.dense(output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MultiHeadAttention, self).get_config()\n",
        "        config.update({'d_model': self.d_model, 'num_heads': self.num_heads})\n",
        "        return config\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(EncoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.mha.d_model,\n",
        "            'num_heads': self.mha.num_heads,\n",
        "            'ff_dim': self.ffn.layers[0].units,\n",
        "            'dropout_rate': self.dropout1.rate\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(DecoderLayer, self).__init__(**kwargs)\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(DecoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.mha1.d_model,\n",
        "            'num_heads': self.mha1.num_heads,\n",
        "            'ff_dim': self.ffn.layers[0].units,\n",
        "            'dropout_rate': self.dropout1.rate\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Inference function for LSTM and Attention models\n",
        "@tf.function(reduce_retracing=True)\n",
        "def predict_step(model, inputs):\n",
        "    return model(inputs, training=False)\n",
        "\n",
        "def generate_title_lstm_attention(model, article_sequence, target_tokenizer, max_title_len):\n",
        "    decoder_input = np.zeros((1, max_title_len - 1))\n",
        "    title = []\n",
        "    vocab_size = min(len(target_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "    for i in range(max_title_len - 1):\n",
        "        predictions = predict_step(model, [article_sequence, decoder_input])\n",
        "        predicted_id = np.argmax(predictions[0, i, :])\n",
        "\n",
        "        # Debug: Cetak predicted_id untuk memeriksa\n",
        "        print(f\"Step {i}, Predicted ID: {predicted_id}\")\n",
        "\n",
        "        # Hentikan jika predicted_id tidak valid atau di luar kosakata\n",
        "        if predicted_id == 0 or predicted_id >= vocab_size:\n",
        "            break\n",
        "\n",
        "        # Cari kata yang sesuai dengan predicted_id\n",
        "        for word, index in target_tokenizer.word_index.items():\n",
        "            if index == predicted_id:\n",
        "                title.append(word)\n",
        "                break\n",
        "        else:\n",
        "            # Jika tidak ditemukan kata, lanjutkan ke langkah berikutnya\n",
        "            continue\n",
        "\n",
        "        decoder_input[0, i] = predicted_id\n",
        "\n",
        "    return ' '.join(title) if title else \"No title generated\"\n",
        "\n",
        "# Inference function for Transformer model\n",
        "def generate_title_transformer(model, article_sequence, target_tokenizer, max_title_len):\n",
        "    decoder_input = np.zeros((1, max_title_len - 1))\n",
        "    title = []\n",
        "    vocab_size = min(len(target_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "    for i in range(max_title_len - 1):\n",
        "        predictions = predict_step(model, [article_sequence, decoder_input])\n",
        "        predicted_id = np.argmax(predictions[0, i, :])\n",
        "\n",
        "        # Debug: Cetak predicted_id untuk memeriksa\n",
        "        print(f\"Transformer Step {i}, Predicted ID: {predicted_id}\")\n",
        "\n",
        "        # Hentikan jika predicted_id tidak valid atau di luar kosakata\n",
        "        if predicted_id == 0 or predicted_id >= vocab_size:\n",
        "            break\n",
        "\n",
        "        # Cari kata yang sesuai dengan predicted_id\n",
        "        for word, index in target_tokenizer.word_index.items():\n",
        "            if index == predicted_id:\n",
        "                title.append(word)\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        decoder_input[0, i] = predicted_id\n",
        "\n",
        "    return ' '.join(title) if title else \"No title generated\"\n",
        "\n",
        "# Main function to generate titles\n",
        "def main(article_text):\n",
        "    # Load tokenizers\n",
        "    try:\n",
        "        with open('input_tokenizer.pkl', 'rb') as f:\n",
        "            input_tokenizer = pickle.load(f)\n",
        "        with open('target_tokenizer.pkl', 'rb') as f:\n",
        "            target_tokenizer = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Tokenizer files (input_tokenizer.pkl, target_tokenizer.pkl) not found.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess article\n",
        "    article_sequence = preprocess_article(article_text, input_tokenizer)\n",
        "\n",
        "    # Load and compile models\n",
        "    try:\n",
        "        lstm_model = tf.keras.models.load_model('lstm_encoder_decoder_model.h5')\n",
        "        lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"LSTM model loaded and compiled.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LSTM model: {e}\")\n",
        "        lstm_model = None\n",
        "\n",
        "    try:\n",
        "        attention_model = tf.keras.models.load_model('encoder_decoder_attention_model.h5')\n",
        "        attention_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"Attention model loaded and compiled.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Attention model: {e}\")\n",
        "        attention_model = None\n",
        "\n",
        "    try:\n",
        "        transformer_model = tf.keras.models.load_model(\n",
        "            'transformer_model.h5',\n",
        "            custom_objects={\n",
        "                'MultiHeadAttention': MultiHeadAttention,\n",
        "                'EncoderLayer': EncoderLayer,\n",
        "                'DecoderLayer': DecoderLayer\n",
        "            }\n",
        "        )\n",
        "        transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"Transformer model loaded and compiled.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Transformer model: {e}\")\n",
        "        transformer_model = None\n",
        "\n",
        "    # Generate titles\n",
        "    print(\"\\nGenerated Titles:\")\n",
        "    print(\"-------------------\")\n",
        "\n",
        "    if lstm_model:\n",
        "        lstm_title = generate_title_lstm_attention(lstm_model, article_sequence, target_tokenizer, MAX_TITLE_LEN)\n",
        "        print(f\"LSTM Encoder-Decoder: {lstm_title}\")\n",
        "    else:\n",
        "        print(\"LSTM Encoder-Decoder: Not available due to loading error\")\n",
        "\n",
        "    if attention_model:\n",
        "        attention_title = generate_title_lstm_attention(attention_model, article_sequence, target_tokenizer, MAX_TITLE_LEN)\n",
        "        print(f\"Encoder-Decoder with Attention: {attention_title}\")\n",
        "    else:\n",
        "        print(\"Encoder-Decoder with Attention: Not available due to loading error\")\n",
        "\n",
        "    if transformer_model:\n",
        "        transformer_title = generate_title_transformer(transformer_model, article_sequence, target_tokenizer, MAX_TITLE_LEN)\n",
        "        print(f\"Transformer: {transformer_title}\")\n",
        "    else:\n",
        "        print(\"Transformer: Not available due to loading error\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Artikel yang diberikan\n",
        "    article = \"\"\"\n",
        "    The global economy is showing signs of recovery after a challenging year. Stock markets in major economies have risen by 10% in the last quarter, driven by strong corporate earnings and optimism about vaccine distribution. However, concerns remain about inflation and supply chain disruptions, which could slow growth in the coming months.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Input Article:\")\n",
        "    print(\"-------------\")\n",
        "    print(article.strip())\n",
        "\n",
        "    # Jalankan fungsi untuk menghasilkan judul\n",
        "    main(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESJS7UNwkFT5",
        "outputId": "2ee56fe1-db32-429d-b62f-b90af2da8087"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Article:\n",
            "-------------\n",
            "The global economy is showing signs of recovery after a challenging year. Stock markets in major economies have risen by 10% in the last quarter, driven by strong corporate earnings and optimism about vaccine distribution. However, concerns remain about inflation and supply chain disruptions, which could slow growth in the coming months.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model loaded and compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention model loaded and compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model loaded and compiled.\n",
            "\n",
            "Generated Titles:\n",
            "-------------------\n",
            "Step 0, Predicted ID: 0\n",
            "LSTM Encoder-Decoder: No title generated\n",
            "Step 0, Predicted ID: 0\n",
            "Encoder-Decoder with Attention: No title generated\n",
            "Transformer Step 0, Predicted ID: 0\n",
            "Transformer: No title generated\n"
          ]
        }
      ]
    }
  ]
}